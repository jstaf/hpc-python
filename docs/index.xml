<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HPC Python</title>
    <link>https://jstaf.github.io/hpc-python/</link>
    <description>Recent content on HPC Python</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 23 Apr 2016 15:21:22 +0200</lastBuildDate>
    
	<atom:link href="https://jstaf.github.io/hpc-python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pipelining with Python</title>
      <link>https://jstaf.github.io/hpc-python/</link>
      <pubDate>Sat, 23 Apr 2016 15:21:22 +0200</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/</guid>
      <description>Python is probably the most versatile language in existence. However one of its most useful features is its ability to tie things together and automate the execution of other programs.
This tutorial focuses on using Python in HPC environments to automate data analysis pipelines with Snakemake. We&amp;rsquo;ll start with the basics and cover everything you need to get started. Some elements of writing performance-oriented code will be covered, but it is not the main focus.</description>
    </item>
    
    <item>
      <title>Basic syntax</title>
      <link>https://jstaf.github.io/hpc-python/basics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/basics/</guid>
      <description>The most basic use of Python is to use it as a fancy calculator. It is very easy to do basic math in Python.
print(5 + 1)  6  Note that we don&amp;rsquo;t always have to use the print() statement. Notice how leaving out print() gives us the same result as above.
5 + 1  6  Python can do all of the normal basic math operations you&amp;rsquo;d expect.</description>
    </item>
    
    <item>
      <title>Scripts and imports</title>
      <link>https://jstaf.github.io/hpc-python/scripts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/scripts/</guid>
      <description>Everything we&amp;rsquo;ve learned so far is pretty cool. But if we want to run a set of commands more than once? How do we write a program in Python?
Python programs are simply a set of Python commands saved in a file. No compiling required! To demonstrate this, let&amp;rsquo;s write our first program! Enter the following text in a text editor and save it under any name you like (Python files are typically given the extension .</description>
    </item>
    
    <item>
      <title>Lists and arrays</title>
      <link>https://jstaf.github.io/hpc-python/lists/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/lists/</guid>
      <description>At the end of the last lesson, we saw noticed that sys.argv gave us a new datastructure: a list. A list is a set of objects enclosed by a set of square brackets ([]).
example = [1, 2, 4, 5] example  [1, 2, 4, 5]  Note that a list can hold any type of item, even other lists!
example = [1, True, None, [&amp;quot;word&amp;quot;, 123], &amp;quot;test&amp;quot;] example  [1, True, None, [&#39;word&#39;, 123], &#39;test&#39;]  We can get different pieces of a list via indexing.</description>
    </item>
    
    <item>
      <title>Storing data with dicts</title>
      <link>https://jstaf.github.io/hpc-python/dicts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/dicts/</guid>
      <description>Dictionaries (also called dicts) are another key datastructure we&amp;rsquo;ll need to use to write a pipeline. In particular, dicts allow efficient key-value storage of any type of data.
To create a dict, we use syntax like the following.
example = {} type(example)  dict  We can then store values in our dict using indexing. The index is referred to as the &amp;ldquo;key&amp;rdquo;, and the stored data is referred to as the &amp;ldquo;value&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Functional programming</title>
      <link>https://jstaf.github.io/hpc-python/functions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/functions/</guid>
      <description>Of course, at some point, we are going to want to define our ownfunctions rather than just use the ones provided by Python and its various modules.
The general syntax for defining a function is as follows:
def function(arg1): # do stuff with arg1 return ans  So, an example function that adds two numbers together might look a little like this:
def adder(x, y): return x + y adder(1, 2)  3  We can also add a default argument (say if we wanted y to be equal to 10 unless we otherwise specified), by using an equals sign and a default value in our function definition:</description>
    </item>
    
    <item>
      <title>Snakemake</title>
      <link>https://jstaf.github.io/hpc-python/sm-intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/sm-intro/</guid>
      <description>Let&amp;rsquo;s imagine that we&amp;rsquo;re interested in testing Zipf&amp;rsquo;s Law in some of our favorite books.
Zipf&amp;rsquo;s Law The most frequently-occurring word occurs approximately twice as often as the second most frequent word. This is [Zipf&amp;rsquo;s Law][zipfs-law]. {: .callout}
We&amp;rsquo;ve compiled our raw data i.e. the books we want to analyze and have prepared several Python scripts that together make up our analysis pipeline.
Let&amp;rsquo;s take quick look at one of the books using the command head books/isles.</description>
    </item>
    
    <item>
      <title>Snakefiles</title>
      <link>https://jstaf.github.io/hpc-python/sm-snakefiles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/sm-snakefiles/</guid>
      <description>Create a file, called Snakefile, with the following content:
# Count words. rule count_words: input: &#39;books/isles.txt&#39;, output: &#39;isles.dat&#39; shell: &#39;python wordcount.py books/isles.txt isles.dat&#39;  This is a build file, which for Snakemake is called a Snakefile - a file executed by Snakemake. Note that aside from a few keyword additions like rule, it follows standard Python 3 syntax.
Let us go through each line in turn:
 # denotes a comment.</description>
    </item>
    
    <item>
      <title>Wildcards</title>
      <link>https://jstaf.github.io/hpc-python/sm-wildcards/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/sm-wildcards/</guid>
      <description>After the exercise at the end of the previous episode, our Snakefile looked like this:
# generate summary table rule zipf_test: input: &#39;abyss.dat&#39;, &#39;last.dat&#39;, &#39;isles.dat&#39; output: &#39;results.txt&#39; shell: &#39;python zipf_test.py abyss.dat isles.dat last.dat &amp;gt; results.txt&#39; rule dats: input: &#39;isles.dat&#39;, &#39;abyss.dat&#39;, &#39;last.dat&#39; # delete everything so we can re-run things rule clean: shell: &#39;rm -f *.dat results.txt&#39; # count words in one of our &amp;quot;books&amp;quot; rule count_words: input: &#39;books/isles.txt&#39; output: &#39;isles.</description>
    </item>
    
    <item>
      <title>Pattern Rules</title>
      <link>https://jstaf.github.io/hpc-python/sm-patterns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/sm-patterns/</guid>
      <description>Our Snakefile still has a ton of repeated content. The rules for each .dat file all do the same thing for the part. We can replace these rules with a single pattern rule which can be used to build any .dat file from a .txt file in books/:
rule count_words: input: wc=&#39;wordcount.py&#39;, book=&#39;books/{file}.txt&#39; output: &#39;{file}.dat&#39; shell: &#39;python {input.wc} {input.book} {output}&#39;  {file} is another arbitrary wildcard, that we can use as a placeholder for any generic book to analyze.</description>
    </item>
    
    <item>
      <title>Pipelines as Python code</title>
      <link>https://jstaf.github.io/hpc-python/sm-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/sm-python/</guid>
      <description>Despite our efforts, our pipeline still has repeated content, for instance the names of output files/dependencies. Our zipf_test rule, for instance, is extremely clunky. What happens if we want to analyze books/sierra.txt as well? We&amp;rsquo;d have to update everything!
rule zipf_test: input: &#39;zipf_test.py&#39;, &#39;abyss.dat&#39;, &#39;last.dat&#39;, &#39;isles.dat&#39; output: &#39;results.txt&#39; shell: &#39;python {input[0]} {input[1]} {input[2]} {input[3]} &amp;gt; {output}&#39;  First, let&amp;rsquo;s cut down on a little bit of the clunkiness of the &amp;ldquo;shell&amp;rdquo; rule.</description>
    </item>
    
    <item>
      <title>Resources and parallelism</title>
      <link>https://jstaf.github.io/hpc-python/sm-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/sm-resources/</guid>
      <description>After the excercises at the end of our last lesson, our Snakefile looks something like this:
# our zipf analysis pipeline DATS = glob_wildcards(&#39;books/{book}.txt&#39;).book rule all: input: &#39;zipf_analysis.tar.gz&#39; # delete everything so we can re-run things rule clean: shell: &#39;&#39;&#39; rm -rf results dats plots rm -f results.txt zipf_analysis.tar.gz &#39;&#39;&#39; # count words in one of our &amp;quot;books&amp;quot; rule count_words: input: wc=&#39;wordcount.py&#39;, book=&#39;books/{file}.txt&#39; output: &#39;dats/{file}.dat&#39; shell: &#39;python {input.wc} {input.book} {output}&#39; # create a plot for each book rule make_plot: input: plotcount=&#39;plotcount.</description>
    </item>
    
    <item>
      <title>Logging</title>
      <link>https://jstaf.github.io/hpc-python/sm-logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/sm-logging/</guid>
      <description>By default, Snakemake prints all output from stderr and stdout from rules. This is useful, but if a failure occurs (or we otherwise need to inspect the logs) it can be extremely difficult to determine what happened or which rule had an issue.
The solution to this issue is to redirect the output from each rule/ set of inputs to a dedicated logfile. We can do this using the log keyword.</description>
    </item>
    
    <item>
      <title>Scaling across a cluster</title>
      <link>https://jstaf.github.io/hpc-python/sm-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/sm-cluster/</guid>
      <description>Right now we have a reasonably effective pipeline that scales nicely on our local computer. However, for the sake of this course, we&amp;rsquo;ll pretend that our workflow actually takes significant computational resources and needs to be run on a cluster.
HPC cluster architecture
Most HPC clusters are run using a scheduler. The scheduler is a piece of software that handles which compute jobs are run when and where. It allows a set of users to share a shared computing system as efficiently as possible.</description>
    </item>
    
    <item>
      <title>Final notes</title>
      <link>https://jstaf.github.io/hpc-python/sm-final-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jstaf.github.io/hpc-python/sm-final-notes/</guid>
      <description>Now that we know how to write and scale a pipeline, here are some tips and tricks for making the process go more smoothly.
&amp;ldquo;snakemake -n&amp;rdquo; is your friend Whenever you edit your Snakefile, run snakemake -n immediately afterwards. This will check for errors and make sure that the pipeline is able to run.
The most common source of errors is a mismatch in filenames (Snakemake doesn&amp;rsquo;t know how to produce a particular output file) - snakemake -n will catch this as long as the troublesome output files haven&amp;rsquo;t already been made.</description>
    </item>
    
  </channel>
</rss>